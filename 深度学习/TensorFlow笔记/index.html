<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="嵌入式开发"><title>TensorFlow笔记 | wzhchen's blog</title><link rel="stylesheet" type="text/css" href="//fonts.neworld.org/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">TensorFlow笔记</h1><a id="logo" href="/.">wzhchen's blog</a><p class="description">学习、记录、分享</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">TensorFlow笔记</h1><div class="post-meta"><a href="/深度学习/TensorFlow笔记/#comments" class="comment-count"></a><p><span class="date">Apr 01, 2019</span><span><a href="/categories/深度学习/" class="category">深度学习</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h1 id="安装TensorFlow"><a href="#安装TensorFlow" class="headerlink" title="安装TensorFlow"></a>安装TensorFlow</h1><p>首先<a href="https://tensorflow.google.cn/" target="_blank" rel="noopener">TensorFlow官网</a>有详细的说明</p>
<p>linux环境比较简单，以下记录是关于Windows的</p>
<p>虽然官网有详细的说明，但实际还是会遇到问题的，如果想做一些改进问题就会更多了，以下是我的安装记录</p>
<ol>
<li><p>安装Python</p>
<p>注意一定要安装64位版本，比如我一不小心就装了个32位版本，只好卸载重新安装</p>
<blockquote>
<p>python -v</p>
<p>….</p>
<p>Python 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 23 2018, 23:31:17) [MSC v.1916 <strong>32 bit</strong> (Intel)] on win32</p>
</blockquote>
<p>点击<a href="https://www.python.org/ftp/python/3.6.8/python-3.6.8-amd64.exe" target="_blank" rel="noopener">这里</a>下载Python3.6.8 64位版本</p>
</li>
<li><p>安装Visual Studio Code</p>
<p><strong>强烈推荐安装</strong>，可以在IDE里编写、调试代码</p>
<p>该IDE里提供的Terminal也比Windows自带的cmd好用，后续地命令可以在这个工具里执行</p>
</li>
<li><p>安装虚拟环境（官方也说了推荐这种方式）</p>
<ol>
<li>首先进入想要创建虚拟环境的目录</li>
<li>执行以下命令，在该目录下创建名为venv的虚拟环境，后续所有包都会安装在这个目录下</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install virtualenv</span><br><span class="line">virtualenv --system-site-packages -p python ./venv</span><br></pre></td></tr></table></figure>
<blockquote>
<p>官方提供的命令会报错，命令中不能使用python3</p>
</blockquote>
</li>
<li><p>连续执行以下几个命令，具体可参考官方说明</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.\venv\Scripts\activate #激活虚拟环境</span><br><span class="line">pip install --upgrade pip #更新&amp;安装pip</span><br><span class="line">pip install --upgrade tensorflow #更新&amp;安装tensorflow</span><br><span class="line"><span class="meta">#</span>测试会用到的一些包</span><br><span class="line">pip install matplotlib</span><br><span class="line">pip install opencv-python</span><br><span class="line">pip install tqdm</span><br></pre></td></tr></table></figure>
<p>如果import cv2报错，则可以换成其它版本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip uninstall opencv-python</span><br><span class="line">pip install opencv-python==3.4.5.20</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol start="5">
<li><p>设置VScode使用python虚拟环境</p>
<ol>
<li><p>在VScode的扩展面板里搜索并安装”Python for VScode“</p>
</li>
<li><p>打开设置（在界面左下角），搜索python.pythonPath</p>
</li>
<li><p>修改Workspace Settings，指定刚才的虚拟环境python的路径，比如</p>
<blockquote>
<p>E:\DL\venv\Scripts\python.exe</p>
</blockquote>
<p>修改完成后，需要重启VScode</p>
</li>
</ol>
</li>
</ol>
<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="shape理解"><a href="#shape理解" class="headerlink" title="shape理解"></a>shape理解</h2><p>shape表示张量各维度的数据长度，比如shape=(1,2,3)，3个数字表示3维</p>
<ul>
<li><p>第1个维度数字是1，表示只第1个维度的数据长度是1</p>
</li>
<li><p>第2个维度数字是2，表示只第2个维度的数据长度是2</p>
<p>第3个维度数字是3，表示只第3个维度的数据长度是3</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[x, y] <span class="comment">#只有一个维度，长度是2，所以这个张量的shape=(2,), 或者(2,0)</span></span><br><span class="line">[[[x,y,z], [x,y,z]]] <span class="comment">#这个张量的shape=(1,2,3)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="CNN原理"><a href="#CNN原理" class="headerlink" title="CNN原理"></a>CNN原理</h2><p><a href="https://www.jianshu.com/p/fe428f0b32c1" target="_blank" rel="noopener">https://www.jianshu.com/p/fe428f0b32c1</a></p>
<p>注：多通道卷积：将每个通道的卷积结果相加，得到1个feature map，而不是多个feature  map</p>
<p>如像素大小12x12, 3通道（rgb）对应的shape为12x12x3，与8个3x3的卷积核卷积后得到的shape为：10x10x8</p>
<h1 id="tf函数"><a href="#tf函数" class="headerlink" title="tf函数"></a>tf函数</h1><h2 id="定义变量相关的函数"><a href="#定义变量相关的函数" class="headerlink" title="定义变量相关的函数"></a>定义变量相关的函数</h2><h3 id="tf-placeholder"><a href="#tf-placeholder" class="headerlink" title="tf.placeholder"></a>tf.placeholder</h3><p>定义占位符</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(<span class="string">"float"</span>,[<span class="keyword">None</span>,<span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>,<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>定义一个输入x，有784个维度，但具体的数据暂未给出</p>
<h3 id="tf-Variable"><a href="#tf-Variable" class="headerlink" title="tf.Variable"></a>tf.Variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)</span><br></pre></td></tr></table></figure>
<p>用于生成一个初始值为<code>initial-value</code>的变量。<strong>必须指定初始化值</strong></p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<p>定义2个矩阵变量，分别是权重和偏置，训练过程就是不段变换这2个矩阵\</p>
<h3 id="tf-get-variable"><a href="#tf-get-variable" class="headerlink" title="tf.get_variable"></a>tf.get_variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = tf.get_variable(name, shape=<span class="keyword">None</span>, dtype=tf.float32, initializer=<span class="keyword">None</span>,</span><br><span class="line">      regularizer=<span class="keyword">None</span>, trainable=<span class="keyword">True</span>, collections=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>获取已存在的变量（要求不仅名字，而且初始化方法等各个参数都一样），如果不存在，就新建一个。<br><strong>可以用各种初始化方法，不用明确指定值。</strong></p>
<h3 id="tf-variable-scope"><a href="#tf-variable-scope" class="headerlink" title="tf.variable_scope"></a>tf.variable_scope</h3><p>设置变量名</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'mynet'</span>):</span><br><span class="line">	b = tf.Variable(tf.zeros([<span class="number">10</span>]), name=<span class="string">'b'</span>)</span><br></pre></td></tr></table></figure>
<p>定义的变量名是：mynet/b</p>
<h3 id="tf-zeros-like"><a href="#tf-zeros-like" class="headerlink" title="tf.zeros_like"></a>tf.zeros_like</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.zeros_like(tensor, dtype=<span class="keyword">None</span>, name=<span class="keyword">None</span>, optimize=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>创建一个所有元素都设置为零的张量，张量的类型与tensor相同</p>
<h3 id="tf-truncated-normal"><a href="#tf-truncated-normal" class="headerlink" title="tf.truncated_normal"></a>tf.truncated_normal</h3><p>产生随机正太分布 </p>
<h2 id="变量操作相关的函数"><a href="#变量操作相关的函数" class="headerlink" title="变量操作相关的函数"></a>变量操作相关的函数</h2><h3 id="tf-matmu"><a href="#tf-matmu" class="headerlink" title="tf.matmu"></a>tf.matmu</h3><p>矩阵乘法</p>
<p>softmax：回归函数，将给定的输入x，权重W和偏置b计算所得值回归到[0-1]的范围</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = tf.nn.softmax(tf.matmul(x,W) + b)</span><br></pre></td></tr></table></figure>
<h3 id="tf-reduce-sum"><a href="#tf-reduce-sum" class="headerlink" title="tf.reduce_sum"></a>tf.reduce_sum</h3><p>累加和</p>
<p>以下代码为求交叉熵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y))</span><br></pre></td></tr></table></figure>
<h3 id="tf-cast"><a href="#tf-cast" class="headerlink" title="tf.cast"></a>tf.cast</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cast(x, dtype, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>将x的数据格式转化成dtype.例如，原来x的数据格式是bool，<br>那么将其转化成float以后，就能够将其转化成0和1的序列。反之也可以</p>
<h3 id="tf-reduce-mean"><a href="#tf-reduce-mean" class="headerlink" title="tf.reduce_mean"></a>tf.reduce_mean</h3><p>求平均值</p>
<h3 id="tf-squeeze"><a href="#tf-squeeze" class="headerlink" title="tf.squeeze"></a>tf.squeeze</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">squeeze(input,axis=<span class="keyword">None</span>,name=<span class="keyword">None</span>,squeeze_dims=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>该函数返回一个张量，这个张量是将原始input中所有维度为1的那些维都删掉的结果<br><code>axis</code>可以用来指定要删掉的为1的维度，<strong>此处要注意指定的维度必须确保其是1，否则会报错</strong></p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  't' 是一个维度是[1, 2, 1, 3, 1, 1]的张量</span></span><br><span class="line">tf.shape(tf.squeeze(t))   <span class="comment"># [2, 3]， 默认删除所有为1的维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 't' 是一个维度[1, 2, 1, 3, 1, 1]的张量</span></span><br><span class="line">tf.shape(tf.squeeze(t, [<span class="number">2</span>, <span class="number">4</span>]))  <span class="comment"># [1, 2, 3, 1]，标号从零开始，只删掉了2和4维的1</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-slice"><a href="#tf-slice" class="headerlink" title="tf.slice"></a>tf.slice</h3><p>这个函数的作用是从输入数据input中提取出一块切片</p>
<ul>
<li><p>切片的尺寸是size，切片的开始位置是begin。</p>
</li>
<li><p>切片的尺寸size表示输出tensor的数据维度，其中size[i]表示在第i维度上面的元素个数。</p>
<p>参考<a href="https://www.jianshu.com/p/71e6ef6c121b" target="_blank" rel="noopener">https://www.jianshu.com/p/71e6ef6c121b</a></p>
</li>
</ul>
<h3 id="tf-concat"><a href="#tf-concat" class="headerlink" title="tf.concat"></a>tf.concat</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.concat(values, axis, name=<span class="string">'concat'</span>)</span><br></pre></td></tr></table></figure>
<p>其中：<br> values应该是一个tensor的list或者tuple。<br> axis则是我们想要连接的维度。<br> tf.concat返回的是连接后的tensor。<br> 比如，如果list中的tensor的shape都是（2，2，2），如果此时的axis为2，即连接第三个维度，那么连接后的shape是（2，2，4），具体表现为对应维度的堆砌。例子如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t1 = [[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]], [[<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">3</span>]]]</span><br><span class="line">t2 = [[[<span class="number">7</span>, <span class="number">4</span>], [<span class="number">8</span>, <span class="number">4</span>]], [[<span class="number">2</span>, <span class="number">10</span>], [<span class="number">15</span>, <span class="number">11</span>]]]</span><br><span class="line">tf.concat([t1, t2], axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<p>输出结果为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor <span class="string">'concat_2:0'</span> shape=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>) dtype=int32&gt;</span><br></pre></td></tr></table></figure>
<p>再sess.run（）一下拿出具体tensor为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">7</span>,  <span class="number">4</span>],</span><br><span class="line">  [ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">8</span>,  <span class="number">4</span>]],</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">4</span>,  <span class="number">4</span>,  <span class="number">2</span>, <span class="number">10</span>],</span><br><span class="line">  [ <span class="number">5</span>,  <span class="number">3</span>, <span class="number">15</span>, <span class="number">11</span>]]]</span><br></pre></td></tr></table></figure>
<p>可见符合（2，2，4）的shape。</p>
<p><strong>tf.train.string_input_producer</strong></p>
<p>这个函数需要传入一个文件名list，系统会自动将它转为一个文件名队列。</p>
<p>此外tf.train.string_input_producer还有两个重要的参数，一个是num_epochs，它就是我们上文中提到的epoch数。另外一个就是shuffle，shuffle是指在一个epoch内文件的顺序是否被打乱。</p>
<h1 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h1><p>tensorboard 作为一款可视化神器，可以说是学习tensorflow时模型训练以及参数可视化的法宝。</p>
<p>而在训练过程中，主要用到了tf.summary()的各类方法，能够保存训练过程以及参数分布图并在tensorboard显示。</p>
<p>参考<a href="https://www.cnblogs.com/huliangwen/p/7445838.html" target="_blank" rel="noopener">TensorFlow框架(2)之TensorBoard详解</a> </p>
<h1 id="TFRecord"><a href="#TFRecord" class="headerlink" title="TFRecord"></a>TFRecord</h1><p>Tfrecord是tensorflow官方推荐的训练数据存储格式，它更容易与网络应用架构相匹配。</p>
<p>Tfrecord本质上是二进制的Protobuf数据，因而其读取、传输的速度更快。Tfrecord文件的每一条记录都是一个<code>tf.train.Example</code>的实例。</p>
<p>使用tfrecord文件格式的另一个好处是数据结构统一，屏蔽了底层的数据结构。在类似于图像分类的任务中，原始数据是各个图片以单独的小文件的形式存在，label又以文件夹的形式存在，处理这样的数据比较麻烦，比如随机打乱，分batch等操作；而所有原始数据转换为一个或几个单独的tfrecord文件后处理起来就会比较方便。</p>
<h2 id="生成tfrecord文件"><a href="#生成tfrecord文件" class="headerlink" title="生成tfrecord文件"></a>生成tfrecord文件</h2><p>何把原始数据转换为tfrecord文件格式，请参考下面的代码片段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_bytes_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(value, list):</span><br><span class="line">        value = [value]</span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(bytes_list=tf.train.BytesList(value=value))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_int64_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(value, list):</span><br><span class="line">        value = [value]</span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(int64_list=tf.train.Int64List(value=value))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立tfrecorder writer</span></span><br><span class="line">writer = tf.python_io.TFRecordWriter(<span class="string">'csv_train.tfrecords'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(train_values.shape[<span class="number">0</span>]):</span><br><span class="line">    image_raw = train_values[i].tostring()</span><br><span class="line">	features=tf.train.Features(feature=&#123;</span><br><span class="line">        <span class="string">'image_raw'</span>:  _bytes_feature([image_raw]),</span><br><span class="line">        <span class="string">'label'</span>: _int64_feature([train_labels[i]])</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment"># build example protobuf</span></span><br><span class="line">    example = tf.train.Example(features=features)</span><br><span class="line">    writer.write(example.SerializeToString())</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h2 id="使用tfrecord文件"><a href="#使用tfrecord文件" class="headerlink" title="使用tfrecord文件"></a>使用tfrecord文件</h2><ol>
<li>定义与保存文件时对应的解析文件方法</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_exmp</span><span class="params">(serial_exmp)</span>:</span></span><br><span class="line">    features=&#123;<span class="string">'image_raw'</span>: tf.FixedLenFeature([<span class="number">3</span>], tf.int64),</span><br><span class="line">              <span class="string">'label'</span>: tf.FixedLenFeature([<span class="number">3</span>],tf.float32)</span><br><span class="line">    &#125;</span><br><span class="line">    feats = tf.parse_single_example(serial_exmp, features=features)</span><br><span class="line">    image = tf.decode_raw(features[<span class="string">'image_raw'</span>], tf.uint8)</span><br><span class="line">    <span class="comment">#根据实际情况对image格式做转换</span></span><br><span class="line">    <span class="comment">#....</span></span><br><span class="line">    label = tf.cast(features[<span class="string">'label'</span>], tf.int32)</span><br><span class="line">    <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><p>使用TFRecordDataset读取tfrecord文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tf_data</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''读取tfrecord数据'''</span></span><br><span class="line">    dataset = tf.data.TFRecordDataset([tf_filename1, tf_filename2])</span><br><span class="line">    dataset = dataset.map(parse_exmp)</span><br><span class="line">    dataset = dataset.shuffle(<span class="number">1000</span>)</span><br><span class="line">    dataset = dataset.repeat(<span class="number">1</span>).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    iterator = dataset.make_one_shot_iterator()</span><br><span class="line">    one_element = iterator.get_next()</span><br><span class="line">	<span class="keyword">return</span> one_element</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用session获取实际的数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">while</span>(<span class="keyword">True</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        result = sess.run(one_element)</span><br><span class="line">        print(result[<span class="number">0</span>], result[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">except</span> tf.errors.OutOfRangeError:</span><br><span class="line">        print(<span class="string">"end!"</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="tf-data的使用"><a href="#tf-data的使用" class="headerlink" title="tf.data的使用"></a>tf.data的使用</h2><p>参考:<a href="https://zhuanlan.zhihu.com/p/38421397" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38421397</a></p>
<h1 id="模型的保存与恢复-Saver"><a href="#模型的保存与恢复-Saver" class="headerlink" title="模型的保存与恢复(Saver)"></a>模型的保存与恢复(Saver)</h1><p>将训练好的模型参数保存起来，以便以后进行验证或测试，这是我们经常要做的事情。tf里面提供模型保存的是tf.train.Saver()模块。</p>
<p>模型保存，先要创建一个Saver对象：如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver=tf.train.Saver()</span><br></pre></td></tr></table></figure>
<p>在创建这个Saver对象的时候，有一个参数我们经常会用到，就是 max_to_keep 参数，这个是用来设置保存模型的个数，默认为5，即 max_to_keep=5，保存最近的5个模型。如果你想每训练一代（epoch)就想保存一次模型，则可以将 max_to_keep设置为None或者0，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver=tf.train.Saver(max_to_keep=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>但是这样做除了多占用硬盘，并没有实际多大的用处，因此不推荐。</p>
<p>当然，如果你只想保存最后一代的模型，则只需要将max_to_keep设置为1即可，即</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver=tf.train.Saver(max_to_keep=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>创建完saver对象后，就可以保存训练好的模型了，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver.save(sess,<span class="string">'model/mnist.ckpt'</span>,global_step=step)</span><br></pre></td></tr></table></figure>
<p> 生成的文件在model目录下，文件名的前缀是<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">第一个参数sess,这个就不用说了。第二个参数设定保存的路径和名字，第三个参数将训练的次数作为后缀加入到模型名字中。</span><br><span class="line"></span><br><span class="line">&gt; saver.save(sess, &apos;my-model&apos;, global_step=0) ==&gt;      filename: &apos;my-model-0&apos;</span><br><span class="line">&gt; ...</span><br><span class="line">&gt; saver.save(sess, &apos;my-model&apos;, global_step=1000) ==&gt; filename: &apos;my-model-1000&apos;</span><br><span class="line"></span><br><span class="line">模型的恢复用的是restore()函数，它需要两个参数restore(sess, save_path)，save_path指的是保存的模型路径。我们可以使用tf.train.latest_checkpoint（）来自动获取最后一次保存的模型。如：</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">model_file=tf.train.latest_checkpoint(&apos;model/&apos;)</span><br><span class="line">saver.restore(sess,model_file)</span><br></pre></td></tr></table></figure></p>
<h1 id="PB文件"><a href="#PB文件" class="headerlink" title="PB文件"></a>PB文件</h1><h2 id="生成pb文件"><a href="#生成pb文件" class="headerlink" title="生成pb文件"></a>生成pb文件</h2><p> tf.graph_util.convert_variables_to_constants函数，会将计算图中的变量取值以常量的形式保存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.platform <span class="keyword">import</span> gfile</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    a = tf.Variable(tf.constant(<span class="number">5.</span>,shape=[<span class="number">1</span>]),name=<span class="string">"a"</span>)</span><br><span class="line">    b = tf.Variable(tf.constant(<span class="number">6.</span>,shape=[<span class="number">1</span>]),name=<span class="string">"b"</span>)</span><br><span class="line">    c = a + b</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment">#导出当前计算图的GraphDef部分</span></span><br><span class="line">    graph_def = tf.get_default_graph().as_graph_def()</span><br><span class="line">    <span class="comment">#保存指定的节点，并将节点值保存为常数</span></span><br><span class="line">    output_graph_def = tf.graph_util.convert_variables_to_constants(sess,graph_def,[<span class="string">'add'</span>])</span><br><span class="line">    <span class="comment">#将计算图写入到模型文件中</span></span><br><span class="line">    model_f = tf.gfile.GFile(<span class="string">"model.pb"</span>,<span class="string">"wb"</span>)</span><br><span class="line">    model_f.write(output_graph_def.SerializeToString())</span><br></pre></td></tr></table></figure>
<h2 id="分析pb文件"><a href="#分析pb文件" class="headerlink" title="分析pb文件"></a>分析pb文件</h2><p> 使用Tensorboard分析pb文件，有两种方法</p>
<p><strong>方法一：</strong></p>
<ul>
<li>利用pb文件恢复计算图</li>
<li>利用Tensorboard查看计算图的结构</li>
</ul>
<p><strong>方法二</strong></p>
<ul>
<li>利用tensorflow提供的tools里的import_pb_to_tensorboard.py这个工具，但是这个工具linux版本的tensorflow没有安装（Win下默认安装），需要的可以去下载<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/tools%5D" target="_blank" rel="noopener">[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/tools]</a> </li>
</ul>
<h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><ol>
<li>从pb文件中恢复计算图</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">model = <span class="string">'model.pb'</span> <span class="comment">#请将这里的pb文件路径改为自己的</span></span><br><span class="line">graph = tf.get_default_graph()</span><br><span class="line">graph_def = graph.as_graph_def()</span><br><span class="line">graph_def.ParseFromString(tf.gfile.GFile(model, <span class="string">'rb'</span>).read())</span><br><span class="line">tf.import_graph_def(graph_def, name=<span class="string">'graph'</span>)</span><br><span class="line">summaryWriter = tf.summary.FileWriter(<span class="string">'log/'</span>, graph)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>利用Tensorboard查看计算图</li>
</ol>
<p>在命令行运行以下命令，启动Tensorboard</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#命令行运行里执行</span><br><span class="line">tensorboard --logdir log/ #这里的路径就是1中最后一行图保存的路径，请根据自己的需要更改</span><br></pre></td></tr></table></figure>
<h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><p>利用tools里面的import_pb_to_tensorboard.py工具</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>命令行</span><br><span class="line">python -m tensorflow.python.tools.import_pb_to_tensorboard --model_dir="your_path/model.pb" --log_dir="your_log_path"  </span><br><span class="line">tensorboard --logdir="your_log_path" #启动tensorboard</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.tools.import_pb_to_tensorboard <span class="keyword">import</span> import_to_tensorboard</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    model = sys.argv[<span class="number">1</span>]</span><br><span class="line">    log_dir = sys.argv[<span class="number">2</span>]</span><br><span class="line">    import_to_tensorboard(model, log_dir) </span><br><span class="line">    <span class="comment">#调用命令</span></span><br><span class="line">    os.system(<span class="string">'tensorboard --logdir='</span>+log_dir) <span class="comment">#启动tensorboard</span></span><br></pre></td></tr></table></figure>
<p>经过查看源码，第二种方法其实是对第一种方法的包装。<br>两种方法是一致的，只不过第二种方法更加便捷。</p>
<h1 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=UTF-8</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#占位符，x:输入，y_:实际结果</span></span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>,[<span class="keyword">None</span>,<span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                        strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#用于调试</span></span><br><span class="line"><span class="comment">#batch = mnist.train.next_batch(50)</span></span><br><span class="line"><span class="comment">#x_image = tf.reshape(batch[0], [-1,28,28,1])</span></span><br><span class="line"><span class="comment">#y_ = batch[1]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#第一层卷积的权重和偏置，卷积核大小5x5</span></span><br><span class="line"><span class="comment">#原始数据只有1个通道</span></span><br><span class="line"><span class="comment">#卷积数量32个，从32个不同的维度来提取特征，将产生32个输出通道</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line"><span class="comment">#将输入（原始数据）转化成4维向量</span></span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line"><span class="comment">#第一层卷积 -&gt; relu -&gt; 池化</span></span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#第二层卷积的权重和偏置，卷积核大小5x5</span></span><br><span class="line"><span class="comment">#上一层有32个输出通道，因此这一层有32个输入通道</span></span><br><span class="line"><span class="comment">#卷积数量64个，从64个不同的维度来提取特征，将产生64个输出通道</span></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line"><span class="comment">#第二层卷积 -&gt; relu -&gt; 池化</span></span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#全连接层的权重和偏置</span></span><br><span class="line"><span class="comment">#上一层有64个输出通道，输出的尺寸是7x7</span></span><br><span class="line"><span class="comment">#设置全连接数量为1024，从1024个维度提取特征，将产生1024个1维度的输出通道</span></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line"><span class="comment">#将输入（第二层输出）转化成2维向量</span></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line"><span class="comment">#计算连接层的结果</span></span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Dropout层，防止或减轻过拟合，一般用在全连接层。</span></span><br><span class="line">keep_prob = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出层的权重和偏置</span></span><br><span class="line"><span class="comment">#上一层（全连接层）有1024个输出通道</span></span><br><span class="line"><span class="comment">#设置10个输出，分别代码数字0-9的概率</span></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"><span class="comment">#计算输出层的结果，矩阵乘+归1化处理</span></span><br><span class="line">y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#成本（交叉熵）随机梯度递减</span></span><br><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始训练</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">    train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">        x:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"step %d, training accuracy %g"</span>%(i, train_accuracy)</span><br><span class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#测试结果</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"test accuracy %g"</span>%accuracy.eval(feed_dict=&#123;</span><br><span class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;)</span><br></pre></td></tr></table></figure>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>学习过程中参考了以下文章</p>
<p><a href="https://www.cnblogs.com/huliangwen/p/7439653.html" target="_blank" rel="noopener">TensorFlow框架(1)之Computational Graph详解</a><br><a href="https://www.cnblogs.com/huliangwen/p/7445838.html" target="_blank" rel="noopener">TensorFlow框架(2)之TensorBoard详解</a><br><a href="https://www.cnblogs.com/huliangwen/p/7455382.html" target="_blank" rel="noopener">TensorFlow框架(3)之MNIST机器学习入门</a><br><a href="https://www.cnblogs.com/huliangwen/p/7460635.html" target="_blank" rel="noopener">TensorFlow框架(4)之CNN卷积神经网络详解</a></p>
</div><div class="tags"><a href="/tags/TensorFlow/">TensorFlow</a><a href="/tags/环境搭建/">环境搭建</a></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/allegro/allegro/" class="pre"></a><a href="/深度学习/MTCNN tensorflow实现/" class="next">MTCNN tensorflow实现</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#安装TensorFlow"><span class="toc-text">安装TensorFlow</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#基础知识"><span class="toc-text">基础知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#shape理解"><span class="toc-text">shape理解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN原理"><span class="toc-text">CNN原理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#tf函数"><span class="toc-text">tf函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#定义变量相关的函数"><span class="toc-text">定义变量相关的函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-placeholder"><span class="toc-text">tf.placeholder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-Variable"><span class="toc-text">tf.Variable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-get-variable"><span class="toc-text">tf.get_variable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-variable-scope"><span class="toc-text">tf.variable_scope</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-zeros-like"><span class="toc-text">tf.zeros_like</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-truncated-normal"><span class="toc-text">tf.truncated_normal</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#变量操作相关的函数"><span class="toc-text">变量操作相关的函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-matmu"><span class="toc-text">tf.matmu</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-reduce-sum"><span class="toc-text">tf.reduce_sum</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-cast"><span class="toc-text">tf.cast</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-reduce-mean"><span class="toc-text">tf.reduce_mean</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-squeeze"><span class="toc-text">tf.squeeze</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-slice"><span class="toc-text">tf.slice</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-concat"><span class="toc-text">tf.concat</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#summary"><span class="toc-text">summary</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#TFRecord"><span class="toc-text">TFRecord</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#生成tfrecord文件"><span class="toc-text">生成tfrecord文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用tfrecord文件"><span class="toc-text">使用tfrecord文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-data的使用"><span class="toc-text">tf.data的使用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#模型的保存与恢复-Saver"><span class="toc-text">模型的保存与恢复(Saver)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PB文件"><span class="toc-text">PB文件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#生成pb文件"><span class="toc-text">生成pb文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分析pb文件"><span class="toc-text">分析pb文件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#方法一"><span class="toc-text">方法一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#方法二"><span class="toc-text">方法二</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#示例代码"><span class="toc-text">示例代码</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参考链接"><span class="toc-text">参考链接</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/芯片方案/mdm9x07/QMI 拨号&网口/">QMI 拨号&网口</a></li><li class="post-list-item"><a class="post-list-link" href="/Linux系统/linux USB host 驱动/">linux USB host驱动</a></li><li class="post-list-item"><a class="post-list-link" href="/Linux系统/linux USB gadget 驱动/">linux gadget 驱动</a></li><li class="post-list-item"><a class="post-list-link" href="/Linux系统/cmdline初始流程/">cmdline初始流程</a></li><li class="post-list-item"><a class="post-list-link" href="/Linux系统/linux根文件系统挂载过程/">linux根文件系统挂载过程</a></li><li class="post-list-item"><a class="post-list-link" href="/芯片方案/mdm9x07/mdm9x07内存使用分析/">mdm9x07内存使用分析</a></li><li class="post-list-item"><a class="post-list-link" href="/allegro/allegro/">allegro/allegro</a></li><li class="post-list-item"><a class="post-list-link" href="/深度学习/TensorFlow笔记/">TensorFlow笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/深度学习/MTCNN tensorflow实现/">MTCNN tensorflow实现</a></li><li class="post-list-item"><a class="post-list-link" href="/深度学习/Caffe笔记/">Caffe笔记</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/ARM架构/">ARM架构</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux内核/">linux内核</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/uboot/">uboot</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂项/">杂项</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/芯片方案/">芯片方案</a><span class="category-list-count">4</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/中断/" style="font-size: 15px;">中断</a> <a href="/tags/cmdline/" style="font-size: 15px;">cmdline</a> <a href="/tags/USB/" style="font-size: 15px;">USB</a> <a href="/tags/rootfs/" style="font-size: 15px;">rootfs</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/caffe2/" style="font-size: 15px;">caffe2</a> <a href="/tags/MTCNN/" style="font-size: 15px;">MTCNN</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/arm/" style="font-size: 15px;">arm</a> <a href="/tags/uboot/" style="font-size: 15px;">uboot</a> <a href="/tags/linux驱动/" style="font-size: 15px;">linux驱动</a> <a href="/tags/串口/" style="font-size: 15px;">串口</a> <a href="/tags/tty/" style="font-size: 15px;">tty</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/环境搭建/" style="font-size: 15px;">环境搭建</a> <a href="/tags/芯片资料/" style="font-size: 15px;">芯片资料</a> <a href="/tags/mdm9x07/" style="font-size: 15px;">mdm9x07</a> <a href="/tags/imx6/" style="font-size: 15px;">imx6</a> <a href="/tags/rmmnet/" style="font-size: 15px;">rmmnet</a> <a href="/tags/QMI/" style="font-size: 15px;">QMI</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">四月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">wzhchen.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>